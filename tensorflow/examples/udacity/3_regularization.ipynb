{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "\n",
    "# copy from Lab2\n",
    "train_subset = 10000\n",
    "\n",
    "logreg_graph = tf.Graph()\n",
    "with logreg_graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  beta = 0.005\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + beta * tf.nn.l2_loss(weights))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=logreg_graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logistic regression with SGD\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "logreg_sgd_graph = tf.Graph()\n",
    "with logreg_sgd_graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  beta = 0.005\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + beta * tf.nn.l2_loss(weights))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=logreg_sgd_graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression summary\n",
    "# original logistic regression w/o sgd: 83.5%, w/ sgd: 86.0%\n",
    "# regularized logistic regression w/o sgd: 88.0%, w/ sgd: 86.9%, beta = 0.05\n",
    "# regularized logistic regression w/o sgd: 88.8%, w/ sgd: 88.6%, beta = 0.01\n",
    "# regularized logistic regression w/o sgd: 88.3%, w/ sgd: 89.0%, beta = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1-layer hidden layer with regularization\n",
    "\n",
    "# Build 1-hidden layer\n",
    "# network:\n",
    "# input[batch_size, image_size * image_size] => \\\n",
    "# w1[batch_size, 1024] => ReLu [batch_size, 1024] => w2[batch_size, 10] => softmax\n",
    "\n",
    "batch_size = 128\n",
    "relu_size = 1024\n",
    "\n",
    "relu_graph = tf.Graph()\n",
    "\n",
    "# hidden layer calculation (right before softmax)\n",
    "def hidden_layer_calc(dataset, weights1, biases1, weights2, biases2):\n",
    "    relu_input = tf.matmul(dataset, weights1) + biases1\n",
    "    relu_output = tf.nn.relu(relu_input)\n",
    "    logits = tf.matmul(relu_output, weights2) + biases2\n",
    "    return logits\n",
    "\n",
    "with relu_graph.as_default():\n",
    "    # pack input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    # layer 1\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, relu_size]))\n",
    "    biases1 = tf.Variable(tf.zeros([relu_size]))\n",
    "    # layer 2\n",
    "    weights2 = tf.Variable(tf.truncated_normal([relu_size, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = hidden_layer_calc(tf_train_dataset, weights1, biases1, weights2, biases2)\n",
    "    beta = .05\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \\\n",
    "        beta * (tf.nn.l2_loss(weights1) / (image_size * image_size * relu_size)) + \\\n",
    "        beta * (tf.nn.l2_loss(weights2) / (relu_size * num_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(hidden_layer_calc(tf_valid_dataset, weights1, biases1, weights2, biases2))\n",
    "    test_prediction  = tf.nn.softmax(hidden_layer_calc(tf_test_dataset, weights1, biases1, weights2, biases2))\n",
    "    \n",
    "    \n",
    "# run it\n",
    "num_steps = 3\n",
    "\n",
    "with tf.Session(graph=relu_graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hidden layer summary (step == 6000)\n",
    "# original hidden layer: Test accuracy: 90.3%\n",
    "# hidden layer with beta = 0.05: Test accuracy: 84.0%\n",
    "# hidden layer with beta = 0.01: Test accuracy: 89.6%, w/o weights dimension normalization\n",
    "# hidden layer with beta = 1:    Test accuracy: 90.0%, w/  weights dimension normalization\n",
    "# hidden layer with beta = 0.5:  Test accuracy: 89.8%, w/  weights dimension normalization\n",
    "# hidden layer with beta = 0.1:  Test accuracy: 90.4%, w/  weights dimension normalization\n",
    "# hidden layer with beta = 5:    Test accuracy: 89.9%, w/  weights dimension normalization\n",
    "# hidden layer with beta = 0.05: Test accuracy: 89.9%, w/  weights dimension normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the num_steps to 10 and Test accuracy: 81.5%\n",
    "# Change the num_steps to 3  and Test accuracy: 42.6%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1-layer hidden layer with dropout\n",
    "\n",
    "# Build 1-hidden layer\n",
    "# network:\n",
    "# input[batch_size, image_size * image_size] => \\\n",
    "# w1[batch_size, 1024] => ReLu [batch_size, 1024] => w2[batch_size, 10] => softmax\n",
    "\n",
    "batch_size = 128\n",
    "relu_size = 1024\n",
    "\n",
    "relu_graph = tf.Graph()\n",
    "\n",
    "# hidden layer calculation (right before softmax)\n",
    "def hidden_layer_calc(dataset, weights1, biases1, weights2, biases2, keep_prob):\n",
    "    relu_input = tf.matmul(dataset, weights1) + biases1\n",
    "    relu_output = tf.nn.relu(relu_input)\n",
    "    relu_do_output = tf.nn.dropout(relu_output, keep_prob)\n",
    "    logits = tf.matmul(relu_do_output, weights2) + biases2\n",
    "    return logits\n",
    "\n",
    "with relu_graph.as_default():\n",
    "    # pack input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    #keep_prob = tf.placeholder(tf.float32)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    # layer 1\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, relu_size]))\n",
    "    biases1 = tf.Variable(tf.zeros([relu_size]))\n",
    "    # layer 2\n",
    "    weights2 = tf.Variable(tf.truncated_normal([relu_size, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = hidden_layer_calc(tf_train_dataset, weights1, biases1, weights2, biases2, 0.5) # keep_prop = .5 for training\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(hidden_layer_calc(tf_valid_dataset, weights1, biases1, weights2, biases2, 1.))\n",
    "    test_prediction  = tf.nn.softmax(hidden_layer_calc(tf_test_dataset, weights1, biases1, weights2, biases2, 1.))\n",
    "\n",
    "# run it\n",
    "num_steps = 6001\n",
    "\n",
    "with tf.Session(graph=relu_graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for  3 batches, normal regularization: 38.5%, drop out: 39.5%\n",
    "# for  6 batches, normal regularization: 72.7%, drop out: 75.6%\n",
    "# for 30 batches, normal regularization: 81.1%, drop out: 82.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multilayer design (Trial 1)\n",
    "# [weights1,biases1] => relu1 => [weights2,biases2] => relu2 => [weights3,biases3]\n",
    "batch_size = 128\n",
    "relu_size = [1024, 10]\n",
    "\n",
    "multi_layer_graph = tf.Graph()\n",
    "\n",
    "# hidden layer calculation (right before softmax)\n",
    "def multi_layer_calc(dataset, weights1, biases1, weights2, biases2, weights3, biases3):\n",
    "    relu_input1 = tf.matmul(dataset,      weights1) + biases1\n",
    "    relu_output1= tf.nn.relu(relu_input1)\n",
    "    relu_input2 = tf.matmul(relu_output1, weights2) + biases2\n",
    "    relu_output2= tf.nn.relu(relu_input2)\n",
    "    logits      = tf.matmul(relu_output1, weights3) + biases3\n",
    "    return logits\n",
    "\n",
    "with multi_layer_graph.as_default():\n",
    "    # pack input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    #keep_prob = tf.placeholder(tf.float32)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, relu_size[0]]))\n",
    "    biases1  = tf.Variable(tf.zeros([relu_size[0]]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([relu_size[0], relu_size[1]]))\n",
    "    biases2  = tf.Variable(tf.zeros([relu_size[1]]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([relu_size[0], num_labels]))\n",
    "    biases3  = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = multi_layer_calc(tf_train_dataset, weights1, biases1, weights2, biases2, weights3, biases3)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    #global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    #learning_rate = tf.train.exponential_decay(0.5, step, 100000, 0.96, staircase=True)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(multi_layer_calc(tf_valid_dataset, weights1, biases1, weights2, biases2, weights3, biases3))\n",
    "    test_prediction  = tf.nn.softmax(multi_layer_calc(tf_test_dataset, weights1, biases1, weights2, biases2, weights3, biases3))\n",
    "\n",
    "# run it\n",
    "num_steps = 6001\n",
    "\n",
    "with tf.Session(graph=multi_layer_graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: nan\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ReluGrad input is not finite. : Tensor had NaN values\n\t [[Node: gradients/Relu_grad/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add)]]\nCaused by op u'gradients/Relu_grad/Relu/CheckNumerics', defined at:\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 151, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-6b3da7bb8070>\", line 49, in <module>\n    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 186, in minimize\n    aggregation_method=aggregation_method)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\n    aggregation_method=aggregation_method)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 445, in gradients\n    in_grads = _AsList(grad_fn(op_wrapper, *out_grads))\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 126, in _ReluGrad\n    t = _VerifyTensor(op.inputs[0], op.name, \"ReluGrad input is not finite.\")\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 119, in _VerifyTensor\n    verify_input = array_ops.check_numerics(t, message=msg)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 48, in check_numerics\n    name=name)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'Relu', defined at:\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n[elided 17 identical lines from previous traceback]\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-6b3da7bb8070>\", line 47, in <module>\n    y_conv = tf.nn.softmax(multi_layer_calc(tf_train_dataset, W1, b1, W2, b2, W_fc1, b_fc1, W_fc2, b_fc2, keep_prob))\n  File \"<ipython-input-7-6b3da7bb8070>\", line 15, in multi_layer_calc\n    h_conv1 = tf.nn.relu(conv2d(dataset_image, weights1, \"Layer1\") + biases1)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 547, in relu\n    return _op_def_lib.apply_op(\"Relu\", features=features, name=name)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6b3da7bb8070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     _, l, predictions = session.run(\n\u001b[0;32m---> 74\u001b[0;31m       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Minibatch loss at step %d: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         raise errors._make_specific_exception(node_def, op, error_message,\n\u001b[0;32m--> 444\u001b[0;31m                                               e.code)\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ReluGrad input is not finite. : Tensor had NaN values\n\t [[Node: gradients/Relu_grad/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add)]]\nCaused by op u'gradients/Relu_grad/Relu/CheckNumerics', defined at:\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 151, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-6b3da7bb8070>\", line 49, in <module>\n    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 186, in minimize\n    aggregation_method=aggregation_method)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\n    aggregation_method=aggregation_method)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 445, in gradients\n    in_grads = _AsList(grad_fn(op_wrapper, *out_grads))\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 126, in _ReluGrad\n    t = _VerifyTensor(op.inputs[0], op.name, \"ReluGrad input is not finite.\")\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 119, in _VerifyTensor\n    verify_input = array_ops.check_numerics(t, message=msg)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 48, in check_numerics\n    name=name)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'Relu', defined at:\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n[elided 17 identical lines from previous traceback]\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-6b3da7bb8070>\", line 47, in <module>\n    y_conv = tf.nn.softmax(multi_layer_calc(tf_train_dataset, W1, b1, W2, b2, W_fc1, b_fc1, W_fc2, b_fc2, keep_prob))\n  File \"<ipython-input-7-6b3da7bb8070>\", line 15, in multi_layer_calc\n    h_conv1 = tf.nn.relu(conv2d(dataset_image, weights1, \"Layer1\") + biases1)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 547, in relu\n    return _op_def_lib.apply_op(\"Relu\", features=features, name=name)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/zhaoyiwei/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "#Multiple layer design2\n",
    "batch_size = 50\n",
    "\n",
    "multi_layer_graph2 = tf.Graph()\n",
    "\n",
    "def conv2d(x, W, in_name):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', use_cudnn_on_gpu=False, name = in_name)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def multi_layer_calc(dataset, weights1, biases1, weights2, biases2, W_fc1, b_fc1, W_fc2, b_fc2, keep_prob):\n",
    "    dataset_image = tf.reshape(dataset, [-1,28,28,1])\n",
    "    # layer1\n",
    "    h_conv1 = tf.nn.relu(conv2d(dataset_image, weights1, \"Layer1\") + biases1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    # layer2\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, weights2, \"Layer2\") + biases2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    # full connected\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    # read out layer\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return logits\n",
    "\n",
    "\n",
    "with multi_layer_graph2.as_default():\n",
    "    # pack input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    #layer1\n",
    "    W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32]))\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64]))\n",
    "    b2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024]))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([1024, 10]))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "    y_conv = tf.nn.softmax(multi_layer_calc(tf_train_dataset, W1, b1, W2, b2, W_fc1, b_fc1, W_fc2, b_fc2, keep_prob))\n",
    "    loss = -tf.reduce_sum(tf_train_labels*tf.log(y_conv))\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(tf_train_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    train_prediction = tf.argmax(y_conv, 1)\n",
    "    valid_prediction = \\\n",
    "        tf.argmax(tf.nn.softmax(multi_layer_calc(tf_valid_dataset, W1, b1, W2, b2, W_fc1, b_fc1, W_fc2, b_fc2, keep_prob)), 1)\n",
    "    test_prediction  = \\\n",
    "        tf.argmax(tf.nn.softmax(multi_layer_calc(tf_test_dataset, W1, b1, W2, b2, W_fc1, b_fc1, W_fc2, b_fc2, keep_prob)), 1)\n",
    "\n",
    "num_steps = 6001\n",
    "\n",
    "with tf.Session(graph=multi_layer_graph2) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      #print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n",
      "offset 0 step 0, training accuracy 0.14\n",
      "offset 5000 step 100, training accuracy 0.72\n",
      "offset 10000 step 200, training accuracy 0.8\n",
      "offset 15000 step 300, training accuracy 0.84\n",
      "offset 20000 step 400, training accuracy 0.86\n",
      "offset 25000 step 500, training accuracy 0.82\n",
      "offset 30000 step 600, training accuracy 0.8\n",
      "offset 35000 step 700, training accuracy 0.84\n",
      "offset 40000 step 800, training accuracy 0.8\n",
      "offset 45000 step 900, training accuracy 0.84\n",
      "offset 50000 step 1000, training accuracy 0.78\n",
      "offset 55000 step 1100, training accuracy 0.86\n",
      "offset 60000 step 1200, training accuracy 0.8\n",
      "offset 65000 step 1300, training accuracy 0.88\n",
      "offset 70000 step 1400, training accuracy 0.84\n",
      "offset 75000 step 1500, training accuracy 0.9\n",
      "offset 80000 step 1600, training accuracy 0.74\n",
      "offset 85000 step 1700, training accuracy 0.88\n",
      "offset 90000 step 1800, training accuracy 0.82\n",
      "offset 95000 step 1900, training accuracy 0.92\n",
      "offset 100000 step 2000, training accuracy 0.76\n",
      "offset 105000 step 2100, training accuracy 0.82\n",
      "offset 110000 step 2200, training accuracy 0.88\n",
      "offset 115000 step 2300, training accuracy 0.8\n",
      "offset 120000 step 2400, training accuracy 0.88\n",
      "offset 125000 step 2500, training accuracy 0.94\n",
      "offset 130000 step 2600, training accuracy 0.9\n",
      "offset 135000 step 2700, training accuracy 0.92\n",
      "offset 140000 step 2800, training accuracy 0.86\n",
      "offset 145000 step 2900, training accuracy 0.9\n",
      "offset 150000 step 3000, training accuracy 0.88\n",
      "offset 155000 step 3100, training accuracy 0.86\n",
      "offset 160000 step 3200, training accuracy 0.92\n",
      "offset 165000 step 3300, training accuracy 0.9\n",
      "offset 170000 step 3400, training accuracy 0.78\n",
      "offset 175000 step 3500, training accuracy 0.8\n",
      "offset 180000 step 3600, training accuracy 0.72\n",
      "offset 185000 step 3700, training accuracy 0.84\n",
      "offset 190000 step 3800, training accuracy 0.86\n",
      "offset 195000 step 3900, training accuracy 0.86\n",
      "offset 50 step 4000, training accuracy 0.9\n",
      "offset 5050 step 4100, training accuracy 0.94\n",
      "offset 10050 step 4200, training accuracy 0.9\n",
      "offset 15050 step 4300, training accuracy 0.88\n",
      "offset 20050 step 4400, training accuracy 0.84\n",
      "offset 25050 step 4500, training accuracy 0.88\n",
      "offset 30050 step 4600, training accuracy 0.86\n",
      "offset 35050 step 4700, training accuracy 0.88\n",
      "offset 40050 step 4800, training accuracy 0.84\n",
      "offset 45050 step 4900, training accuracy 0.92\n",
      "offset 50050 step 5000, training accuracy 0.88\n",
      "offset 55050 step 5100, training accuracy 0.86\n",
      "offset 60050 step 5200, training accuracy 0.88\n",
      "offset 65050 step 5300, training accuracy 0.88\n",
      "offset 70050 step 5400, training accuracy 0.92\n",
      "offset 75050 step 5500, training accuracy 0.92\n",
      "offset 80050 step 5600, training accuracy 0.84\n",
      "offset 85050 step 5700, training accuracy 0.92\n",
      "offset 90050 step 5800, training accuracy 0.88\n",
      "offset 95050 step 5900, training accuracy 0.88\n",
      "offset 100050 step 6000, training accuracy 0.94\n",
      "offset 105050 step 6100, training accuracy 0.86\n",
      "offset 110050 step 6200, training accuracy 0.8\n",
      "offset 115050 step 6300, training accuracy 0.86\n",
      "offset 120050 step 6400, training accuracy 0.9\n",
      "offset 125050 step 6500, training accuracy 0.9\n",
      "offset 130050 step 6600, training accuracy 0.86\n",
      "offset 135050 step 6700, training accuracy 0.92\n",
      "offset 140050 step 6800, training accuracy 0.92\n",
      "offset 145050 step 6900, training accuracy 0.98\n",
      "offset 150050 step 7000, training accuracy 0.82\n",
      "offset 155050 step 7100, training accuracy 0.94\n",
      "offset 160050 step 7200, training accuracy 0.9\n",
      "offset 165050 step 7300, training accuracy 0.94\n",
      "offset 170050 step 7400, training accuracy 0.9\n",
      "offset 175050 step 7500, training accuracy 0.9\n",
      "offset 180050 step 7600, training accuracy 0.86\n",
      "offset 185050 step 7700, training accuracy 0.96\n",
      "offset 190050 step 7800, training accuracy 0.9\n",
      "offset 195050 step 7900, training accuracy 0.84\n",
      "offset 100 step 8000, training accuracy 0.92\n",
      "offset 5100 step 8100, training accuracy 0.9\n",
      "offset 10100 step 8200, training accuracy 0.92\n",
      "offset 15100 step 8300, training accuracy 0.96\n",
      "offset 20100 step 8400, training accuracy 0.9\n",
      "offset 25100 step 8500, training accuracy 0.9\n",
      "offset 30100 step 8600, training accuracy 0.94\n",
      "offset 35100 step 8700, training accuracy 0.94\n",
      "offset 40100 step 8800, training accuracy 0.9\n",
      "offset 45100 step 8900, training accuracy 0.86\n",
      "offset 50100 step 9000, training accuracy 0.96\n",
      "offset 55100 step 9100, training accuracy 0.9\n",
      "offset 60100 step 9200, training accuracy 0.86\n",
      "offset 65100 step 9300, training accuracy 0.84\n",
      "offset 70100 step 9400, training accuracy 0.92\n",
      "offset 75100 step 9500, training accuracy 0.88\n",
      "offset 80100 step 9600, training accuracy 0.94\n",
      "offset 85100 step 9700, training accuracy 0.88\n",
      "offset 90100 step 9800, training accuracy 0.88\n",
      "offset 95100 step 9900, training accuracy 0.96\n",
      "offset 100100 step 10000, training accuracy 0.92\n",
      "offset 105100 step 10100, training accuracy 0.88\n",
      "offset 110100 step 10200, training accuracy 0.86\n",
      "offset 115100 step 10300, training accuracy 0.96\n",
      "offset 120100 step 10400, training accuracy 0.86\n",
      "offset 125100 step 10500, training accuracy 0.88\n",
      "offset 130100 step 10600, training accuracy 0.96\n",
      "offset 135100 step 10700, training accuracy 0.92\n",
      "offset 140100 step 10800, training accuracy 0.96\n",
      "offset 145100 step 10900, training accuracy 0.96\n",
      "offset 150100 step 11000, training accuracy 0.9\n",
      "offset 155100 step 11100, training accuracy 0.92\n",
      "offset 160100 step 11200, training accuracy 0.86\n",
      "offset 165100 step 11300, training accuracy 0.9\n",
      "offset 170100 step 11400, training accuracy 0.96\n",
      "offset 175100 step 11500, training accuracy 0.94\n",
      "offset 180100 step 11600, training accuracy 0.94\n",
      "offset 185100 step 11700, training accuracy 0.96\n",
      "offset 190100 step 11800, training accuracy 0.92\n",
      "offset 195100 step 11900, training accuracy 0.88\n",
      "offset 150 step 12000, training accuracy 0.92\n",
      "offset 5150 step 12100, training accuracy 0.86\n",
      "offset 10150 step 12200, training accuracy 0.9\n",
      "offset 15150 step 12300, training accuracy 0.9\n",
      "offset 20150 step 12400, training accuracy 0.92\n",
      "offset 25150 step 12500, training accuracy 0.88\n",
      "offset 30150 step 12600, training accuracy 0.94\n",
      "offset 35150 step 12700, training accuracy 0.92\n",
      "offset 40150 step 12800, training accuracy 0.9\n",
      "offset 45150 step 12900, training accuracy 0.9\n",
      "offset 50150 step 13000, training accuracy 0.96\n",
      "offset 55150 step 13100, training accuracy 0.92\n",
      "offset 60150 step 13200, training accuracy 0.96\n",
      "offset 65150 step 13300, training accuracy 0.96\n",
      "offset 70150 step 13400, training accuracy 0.92\n",
      "offset 75150 step 13500, training accuracy 0.9\n",
      "offset 80150 step 13600, training accuracy 0.84\n",
      "offset 85150 step 13700, training accuracy 0.9\n",
      "offset 90150 step 13800, training accuracy 0.88\n",
      "offset 95150 step 13900, training accuracy 0.9\n",
      "offset 100150 step 14000, training accuracy 0.88\n",
      "offset 105150 step 14100, training accuracy 0.92\n",
      "offset 110150 step 14200, training accuracy 0.98\n",
      "offset 115150 step 14300, training accuracy 0.9\n",
      "offset 120150 step 14400, training accuracy 0.88\n",
      "offset 125150 step 14500, training accuracy 0.94\n",
      "offset 130150 step 14600, training accuracy 0.88\n",
      "offset 135150 step 14700, training accuracy 0.98\n",
      "offset 140150 step 14800, training accuracy 0.9\n",
      "offset 145150 step 14900, training accuracy 0.88\n",
      "offset 150150 step 15000, training accuracy 0.92\n",
      "offset 155150 step 15100, training accuracy 0.98\n",
      "offset 160150 step 15200, training accuracy 0.94\n",
      "offset 165150 step 15300, training accuracy 0.96\n",
      "offset 170150 step 15400, training accuracy 0.94\n",
      "offset 175150 step 15500, training accuracy 0.88\n",
      "offset 180150 step 15600, training accuracy 0.94\n",
      "offset 185150 step 15700, training accuracy 0.94\n",
      "offset 190150 step 15800, training accuracy 0.94\n",
      "offset 195150 step 15900, training accuracy 0.94\n",
      "offset 200 step 16000, training accuracy 0.88\n",
      "offset 5200 step 16100, training accuracy 0.88\n",
      "offset 10200 step 16200, training accuracy 0.88\n",
      "offset 15200 step 16300, training accuracy 0.96\n",
      "offset 20200 step 16400, training accuracy 0.9\n",
      "offset 25200 step 16500, training accuracy 0.98\n",
      "offset 30200 step 16600, training accuracy 0.92\n",
      "offset 35200 step 16700, training accuracy 0.94\n",
      "offset 40200 step 16800, training accuracy 0.94\n",
      "offset 45200 step 16900, training accuracy 0.92\n",
      "offset 50200 step 17000, training accuracy 0.88\n",
      "offset 55200 step 17100, training accuracy 0.96\n",
      "offset 60200 step 17200, training accuracy 0.84\n",
      "offset 65200 step 17300, training accuracy 0.96\n",
      "offset 70200 step 17400, training accuracy 0.96\n",
      "offset 75200 step 17500, training accuracy 0.9\n",
      "offset 80200 step 17600, training accuracy 0.94\n",
      "offset 85200 step 17700, training accuracy 0.88\n",
      "offset 90200 step 17800, training accuracy 0.86\n",
      "offset 95200 step 17900, training accuracy 0.94\n",
      "offset 100200 step 18000, training accuracy 0.94\n",
      "offset 105200 step 18100, training accuracy 0.94\n",
      "offset 110200 step 18200, training accuracy 0.84\n",
      "offset 115200 step 18300, training accuracy 0.98\n",
      "offset 120200 step 18400, training accuracy 0.98\n",
      "offset 125200 step 18500, training accuracy 0.96\n",
      "offset 130200 step 18600, training accuracy 0.96\n",
      "offset 135200 step 18700, training accuracy 0.96\n",
      "offset 140200 step 18800, training accuracy 0.96\n",
      "offset 145200 step 18900, training accuracy 0.9\n",
      "offset 150200 step 19000, training accuracy 0.92\n",
      "offset 155200 step 19100, training accuracy 0.94\n",
      "offset 160200 step 19200, training accuracy 0.94\n",
      "offset 165200 step 19300, training accuracy 0.9\n",
      "offset 170200 step 19400, training accuracy 0.96\n",
      "offset 175200 step 19500, training accuracy 0.9\n",
      "offset 180200 step 19600, training accuracy 0.92\n",
      "offset 185200 step 19700, training accuracy 0.9\n",
      "offset 190200 step 19800, training accuracy 0.96\n",
      "offset 195200 step 19900, training accuracy 0.94\n",
      "offset 250 step 20000, training accuracy 0.98\n",
      "test accuracy 0.9659\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "def conv2d(x, W, in_name):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "labels  = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "#tf_valid_dataset = tf.constant(valid_dataset)\n",
    "#tf_test_dataset  = tf.constant(test_dataset)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "dataset_image = tf.reshape(dataset, [-1,28,28,1])\n",
    "# layer1\n",
    "h_conv1 = tf.nn.relu(conv2d(dataset_image, W1, \"Layer1\") + b1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "# layer2\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W2, \"Layer2\") + b2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "# full connected\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "# read out layer\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(labels*tf.log(y_conv))\n",
    "train_step_m = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "#train_step_m = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(labels,1))\n",
    "accuracy_m = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "num_steps = 20001\n",
    "with sess.as_default():\n",
    "    for i in range(num_steps):\n",
    "      #batch = mnist.train.next_batch(50)\n",
    "      offset = (i * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      if i%100 == 0:\n",
    "        train_accuracy = accuracy_m.eval(feed_dict={dataset: batch_data, labels: batch_labels, keep_prob: 1.0})\n",
    "        print(\"offset %d step %d, training accuracy %g\" % (offset, i, train_accuracy))\n",
    "      train_step_m.run(feed_dict={dataset: batch_data, labels: batch_labels, keep_prob: 0.5})\n",
    "\n",
    "    print(\"test accuracy %g\" % \\\n",
    "          accuracy_m.eval(feed_dict={dataset: test_dataset, labels: test_labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
